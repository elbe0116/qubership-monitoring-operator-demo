name: Install Monitoring

on:
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment Name"
        required: false
        default: "dev"
      test_tags:
        description: "Test tags to run (e.g. smoke, grafana, or empty for default tags)"
        required: false
        default: ""
      operator_type:
        description: "Operator type (prometheus-operator or victoriametrics-operator)"
        required: false
        default: "victoriametrics-operator"

jobs:
  Install-Monitoring:
    environment: "${{github.event.inputs.environment}}"
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: read
      actions: read

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Listing downloaded files
        run: |
          pwd
          ls -R ./

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.32.0'

      - name: Setup Helm
        uses: azure/setup-helm@v3

      - name: Verify tools
        run: |
          kubectl version --client
          helm version
          aws --version

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION || 'us-east-1' }}

      - name: Configure kubeconfig for EKS
        run: |
          aws eks update-kubeconfig --region ${{ vars.AWS_REGION || 'us-east-1' }} --name ${{ vars.AWS_CLUSTERNAME }}

      - name: Verify cluster access
        run: kubectl get nodes

      - name: Install Monitoring
        run: |
          echo "Clean previously installed Monitoring"
          
          # First, uninstall Helm release if exists
          echo "Uninstalling Helm release..."
          helm uninstall monitoring -n ${{ vars.MONITORING_INSTALL_NAMESPACE }} --wait --timeout=120s || true
          
          # Delete namespace
          echo "Deleting namespace..."
          kubectl delete ns ${{ vars.MONITORING_INSTALL_NAMESPACE }} --timeout=180s --wait=false || true
          
          # Wait for namespace to be fully deleted
          echo "Waiting for namespace deletion to complete..."
          for i in {1..90}; do
            if ! kubectl get ns ${{ vars.MONITORING_INSTALL_NAMESPACE }} 2>/dev/null; then
              echo "Namespace deleted successfully"
              break
            fi
            
            # Check if namespace is stuck in Terminating state
            NS_STATUS=$(kubectl get ns ${{ vars.MONITORING_INSTALL_NAMESPACE }} -o jsonpath='{.status.phase}' 2>/dev/null || echo "")
            echo "Waiting... ($i/90) - Namespace status: $NS_STATUS"
            
            # If stuck in Terminating for too long, try to force delete
            if [ "$i" -gt 30 ] && [ "$NS_STATUS" == "Terminating" ]; then
              echo "Namespace stuck in Terminating state, attempting to force delete..."
              
              # Remove finalizers from all resources in the namespace
              echo "Removing finalizers from namespace resources..."
              kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --ignore-not-found -n ${{ vars.MONITORING_INSTALL_NAMESPACE }} -o name | \
                xargs -n 1 kubectl patch -n ${{ vars.MONITORING_INSTALL_NAMESPACE }} -p '{"metadata":{"finalizers":[]}}' --type=merge 2>/dev/null || true
              
              # Remove finalizers from namespace itself
              echo "Removing finalizers from namespace..."
              kubectl get ns ${{ vars.MONITORING_INSTALL_NAMESPACE }} -o json | \
                jq '.spec.finalizers = []' | \
                kubectl replace --raw "/api/v1/namespaces/${{ vars.MONITORING_INSTALL_NAMESPACE }}/finalize" -f - || true
              
              # Give it a moment to process
              sleep 5
            fi
            
            sleep 2
          done
          
          # Final check - if namespace still exists, fail
          if kubectl get ns ${{ vars.MONITORING_INSTALL_NAMESPACE }} 2>/dev/null; then
            echo "ERROR: Namespace still exists after timeout!"
            kubectl get ns ${{ vars.MONITORING_INSTALL_NAMESPACE }} -o yaml
            exit 1
          fi
          
          # Additional cleanup: ensure no node-exporter pods are still running with hostPort 9900
          echo "Checking for any remaining node-exporter pods..."
          kubectl get pods --all-namespaces -o wide | grep node-exporter || echo "No node-exporter pods found"
          
          echo "Namespace to install: ${{ vars.MONITORING_INSTALL_NAMESPACE }}"
          
          # Create namespace
          echo "Creating namespace..."
          kubectl create namespace ${{ vars.MONITORING_INSTALL_NAMESPACE }}
          
          # Create GHCR secret for pulling private images
          echo "Creating GHCR credentials secret..."
          kubectl create secret docker-registry ghcr-secret \
            --docker-server=ghcr.io \
            --docker-username=${{ github.actor }} \
            --docker-password=${{ secrets.GITHUB_TOKEN }} \
            --namespace=${{ vars.MONITORING_INSTALL_NAMESPACE }}
          
          echo "GHCR credentials configured"
          
          # Build helm arguments dynamically (only add non-empty values)
          HELM_ARGS=""
          
          # Prepare Tag and Group for custom image
          TAG_NAME=$(echo ${{ github.ref_name }} | sed 's@/@_@g')
          GITHUB_GROUP=$(echo ${{ github.repository_owner }} | tr '[:upper:]' '[:lower:]')
          
          # Enable integration tests and set test tags
          HELM_ARGS="$HELM_ARGS --set integrationTests.install=true"
          if [ -n "${{ github.event.inputs.test_tags }}" ]; then
            HELM_ARGS="$HELM_ARGS --set integrationTests.tags=${{ github.event.inputs.test_tags }}"
          fi
          
          # Use custom built image from current branch
          HELM_ARGS="$HELM_ARGS --set integrationTests.image=ghcr.io/${GITHUB_GROUP}/qubership-monitoring-int-tests:${TAG_NAME}"
          
          # Add GHCR credentials for pulling private images
          HELM_ARGS="$HELM_ARGS --set integrationTests.imagePullSecrets[0].name=ghcr-secret"
          
          # Secrets - S3 credentials for test results upload
          [ -n "${{ secrets.S3_AWS_ACCESS_KEY_ID }}" ] && HELM_ARGS="$HELM_ARGS --set integrationTests.atpStorage.username=${{ secrets.S3_AWS_ACCESS_KEY_ID }}"
          [ -n "${{ secrets.S3_AWS_SECRET_ACCESS_KEY }}" ] && HELM_ARGS="$HELM_ARGS --set integrationTests.atpStorage.password=${{ secrets.S3_AWS_SECRET_ACCESS_KEY }}"
          
          # Variables (only add if set, otherwise use values.yaml defaults)
          [ -n "${{ vars.ATP_STORAGE_PROVIDER }}" ] && HELM_ARGS="$HELM_ARGS --set integrationTests.atpStorage.provider=${{ vars.ATP_STORAGE_PROVIDER }}"
          [ -n "${{ vars.ATP_STORAGE_SERVER_URL }}" ] && HELM_ARGS="$HELM_ARGS --set integrationTests.atpStorage.serverUrl=${{ vars.ATP_STORAGE_SERVER_URL }}\""
          [ -n "${{ vars.ATP_STORAGE_SERVER_UI_URL }}" ] && HELM_ARGS="$HELM_ARGS --set integrationTests.atpStorage.serverUiUrl=${{ vars.ATP_STORAGE_SERVER_UI_URL }}\""
          [ -n "${{ vars.ATP_STORAGE_BUCKET }}" ] && HELM_ARGS="$HELM_ARGS --set integrationTests.atpStorage.bucket=${{ vars.ATP_STORAGE_BUCKET }}\""
          [ -n "${{ vars.AWS_REGION }}" ] && HELM_ARGS="$HELM_ARGS --set integrationTests.atpStorage.region=${{ vars.AWS_REGION }}\""
          [ -n "${{ vars.ATP_REPORT_VIEW_UI_URL }}" ] && HELM_ARGS="$HELM_ARGS --set integrationTests.atpReportViewUiUrl=${{ vars.ATP_REPORT_VIEW_UI_URL }}\""
          
          # Environment name (use input or variable)
          ENV_NAME="${{ vars.ENVIRONMENT_NAME }}"
          [ -z "$ENV_NAME" ] && ENV_NAME="${{ github.event.inputs.environment }}"
          [ -n "$ENV_NAME" ] && HELM_ARGS="$HELM_ARGS --set integrationTests.environmentName=$ENV_NAME"
          
          # AWS EKS configuration for smaller clusters
          cat > /tmp/aws-values.yaml << 'EOF'
          integrationTests:
            resources:
              requests:
                memory: 256Mi
                cpu: 100m
              limits:
                memory: 512Mi
                cpu: 200m
            securityContext:
              fsGroup: 1000
              runAsUser: 1000
          
          # VictoriaMetrics components configuration - use AWS ECR to avoid Docker Hub rate limits
          victoriametrics:
            vmOperator:
              image: 442426885383.dkr.ecr.us-east-1.amazonaws.com/helm-charts-test-repo:victoriametrics-operator-v0.65.0
              configReloaderImage: 442426885383.dkr.ecr.us-east-1.amazonaws.com/helm-charts-test-repo:victoriametrics-operator-config-reloader-v0.65.0
            vmAgent:
              image: 442426885383.dkr.ecr.us-east-1.amazonaws.com/helm-charts-test-repo:victoriametrics-vmagent-v1.130.0
            vmAlert:
              image: 442426885383.dkr.ecr.us-east-1.amazonaws.com/helm-charts-test-repo:victoriametrics-vmalert-v1.130.0
            vmAlertManager:
              image: 442426885383.dkr.ecr.us-east-1.amazonaws.com/helm-charts-test-repo:prom-alertmanager-v0.28.1
            vmAuth:
              image: 442426885383.dkr.ecr.us-east-1.amazonaws.com/helm-charts-test-repo:victoriametrics-vmauth-v1.130.0
            vmSingle:
              image: 442426885383.dkr.ecr.us-east-1.amazonaws.com/helm-charts-test-repo:victoriametrics-victoria-metrics-v1.130.0
          
          # Node-exporter configuration for AWS EKS
          nodeExporter:
            # Use AWS ECR image to avoid Docker Hub rate limits
            image:
              repository: 442426885383.dkr.ecr.us-east-1.amazonaws.com/helm-charts-test-repo
              tag: node-exporter-1.9.0
              pullPolicy: IfNotPresent
            # Remove nodeAffinity constraints that may conflict with AWS node labels
            affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: kubernetes.io/os
                      operator: In
                      values:
                      - linux
            # Ensure tolerations for all AWS EKS node taints
            tolerations:
            - operator: Exists
          EOF
          HELM_ARGS="$HELM_ARGS -f /tmp/aws-values.yaml"
          
          # Install Prometheus or VictoriaMetrics operator
          if [ "${{ github.event.inputs.operator_type }}" == "prometheus-operator" ]; then
            HELM_ARGS="$HELM_ARGS --set prometheus.install=true"
            HELM_ARGS="$HELM_ARGS --set victoriametrics.install=false"
          else
            HELM_ARGS="$HELM_ARGS --set prometheus.install=false"
            HELM_ARGS="$HELM_ARGS --set victoriametrics.install=true"
          fi
          
          # Install Grafana for tests
          HELM_ARGS="$HELM_ARGS --set grafana.install=true"
          
          echo "Helm additional args: $HELM_ARGS"
          
          # Debug: show key values from values.yaml
          echo "=== Debug: Key values ==="
          grep -E "^\s*(install|enabled):" ./charts/qubership-monitoring-operator/values.yaml | head -20
          echo "========================="
          
          helm install --namespace=${{ vars.MONITORING_INSTALL_NAMESPACE }} \
            --create-namespace \
            -f ./charts/qubership-monitoring-operator/values.yaml \
            monitoring ./charts/qubership-monitoring-operator \
            $HELM_ARGS
